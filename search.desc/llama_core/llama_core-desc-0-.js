searchState.loadedDescShard("llama_core", 0, "Llama Core, abbreviated as <code>llama-core</code>, defines a set of …\nWrapper of the <code>wasmedge_wasi_nn::Graph</code> struct\nModel metadata\nBuilder for the <code>Metadata</code> struct\nVersion info of the <code>wasi-nn_ggml</code> plugin, including the …\nRunning mode\nGet the alias of the model\nDefine APIs for chat completion.\nDefine APIs for completions.\nCompute the inference on the given inputs.\nCompute the inference on the given inputs.\nDefine APIs for computing embeddings.\nError types for the Llama Core library.\nClear the computation context.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCopy output tensor to out_buffer, return the output’s …\nCopy output tensor to out_buffer, return the output’s …\nGet the plugin info\nPath to the image file for llava\nInitialize the core context\nInitialize the core context for RAG scenarios.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\npath to the multimodal projector file for llava\nDefine APIs for querying models.\nGet the name of the model\nCreate a new computation graph from the given metadata.\nGet the prompt template type\nDefine APIs for RAG operations.\nReturn the current running mode.\nSet input uses the data, not only u8, but also f32, i32, …\nUpdate metadata\nDefine utility functions.\nProcesses a chat-completion request and returns either a …\nProcesses a chat-completion request and returns a …\nProcesses a chat-completion request and returns …\nGiven a prompt, the model will return one or more …\nGet the dimension of the embedding model.\nCompute embeddings for the given input.\nErrors thrown by the wasi-nn-ggml plugin and runtime.\nError types for wasi-nn errors.\nErrors in the model inference.\nErrors in the model inference in the stream mode.\nErrors in cleaning up the computation context in the …\nErrors in getting the output tensor.\nErrors in getting the output tensor in the stream mode.\nErrors in Context initialization.\nError types for the Llama Core library.\nErrors in General operation.\nErrors in setting the input tensor.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nLists models available\nGenerate a list of chunks from a given text. Each chunk …\nConvert document chunks to embeddings.\nConvert a query to embeddings.\nRetrieve similar points from the Qdrant server using the …\nReturn the names of the chat models.\nGet the chat prompt template type from the given model …\nReturn the names of the embedding models.")